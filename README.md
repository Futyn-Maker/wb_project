# wb_project

В данном репозитории находится пример реализации вопросно-ответной системы для сотрудников пунктов выдачи заказов крупного маркетплейса. Система использует RAG-подход и основана на следующих технологиях:

* Фреймворк: [Haystack](https://haystack.deepset.ai/);
* Эмбеддер: модель [multilingual-e5-large-instruct](https://huggingface.co/intfloat/multilingual-e5-large-instruct), [дотренированная](https://huggingface.co/Futyn-Maker/wb_questions) на парах похожих вопросов из датасета Wildberries, т.е. таких, на которые были даны одинаковые ответы;
* LLM: [Дообученная](https://huggingface.co/Futyn-Maker/saiga_llama3_8b_wildberries) на парах "Вопрос-ответ" из датасета Wildberries [Saiga3](https://huggingface.co/IlyaGusev/saiga_llama3_8b), точнее её [Llama.cpp-версия](https://huggingface.co/Futyn-Maker/saiga_llama3_8b_wildberries_4bit_gguf);
* FastAPI+PostgreSQL - для функционирования микросервиса;
* [PGVector](https://github.com/pgvector/pgvector) - расширение для PostgreSQL для хранения в базе эмбеддингов документов (только для локальной версии).

## Использование

Система реализована в двух режимах: в виде Jupyter-ноутбука для Google Colab и в виде микросервиса на FastAPI с минималистичным интерфейсом. Второй вариант подходит для более продолжительной и надёжной работы системы и не использует видеокарту для инференса LLM.

### Способ 1: запуск в Google Colab

1. [Откройте тетрадку с реализованной системой в Google Colab](https://colab.research.google.com/github/Futyn-Maker/wb_project/blob/main/helper_rag.ipynb);
2. Убедитесь, что в качестве среды выполнения установлена среда с GPU;
3. Поместите Excel-файлы с оригинальными данными в сессионное хранилище и выполните первые две ячейки; это создаст каталог `data` и поместит туда ваши данные;
4. Последовательно выполните все остальные ячейки, дождитесь, пока установятся зависимости и подготовятся данные, в т.ч. создадутся эмбеддинги для документов.
5. Во фрейме в последней ячейке отобразиться интерфейс Gradio. В появившемся поле можно задавать вопросы и получать ответы;
6. Пока запущена среда и ячейка с Gradio, вы можете поделиться публичной ссылкой для демонстрации модели всем желающим; ссылка будет расположена в информации в верху фрейма.

### Способ 2: локальный запуск

> Внимание! В этом режиме ответы с помощью LLM генерируются существенно медленно, поскольку не используется видеокарта. Кроме того, если в системе нет видеокарты, вычисление эмбеддингов для документов может занимать до 40 минут. Для работы системы необходим дистрибутив Linux, можно WSL.

1. Склонируйте репозиторий и перейдите в каталог проекта:

```bash
git clone https://github.com/Futyn-Maker/wb_project.git
cd wb_project
```

2. Создайте каталог `data` и поместите туда оригинальные данные в виде XLSX-таблиц, не меняя их названия.

```bash
mkdir data
```

3. Установите переменные среды:

```bash
cp .env.example .env
nano .env
```

Здесь укажите параметры для подключения к вашей PostgreSQL-базе: имя пользователя, пароль, название базы.

4. Если у вас отсутствует модуль venv, установите его: `sudo apt install python3-venv`.
5. Запустите скрипт подготовки среды и данных, дождитесь, пока он завершит работу - это может занять продолжительное время:

```bash
bash prepare.sh
```

Этот скрипт:

* Создаст и активирует новую виртуальную среду;
* Установит зависимости;
* Установит PostgreSQL, если ранее сервис отсутствовал;
* Создаст указанного в `.env` пользователя и базу данных;
* Установит PGVector;
* Запустит серию скриптов для подготовки данных и создания эмбеддингов документов;
* Скачает дообученную языковую модель

6. Запустите сервис:

```bash
bash run.sh
```

7. Ищите интерфейс на http://127.0.0.1:8000/static/index.html;
8. Если необходимо обратиться напрямую к API, делается это следующим образом:

```
http://127.0.0.1:8000/get_answer/?question=Привет!
```
